---
title: "Practical Machine Learning Final Project"
author: "Tomislav Kralj"
date: "April 2023"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(lattice)
library(ggplot2)
library(kernlab)
library(rattle)
library(corrplot)
library(rpart)
library(randomForest)
set.seed(1234)
```

## Introduction

The goal of this project is to use the data from accelerometers placed on the following locations: belt, forearm, arm, and dumbell. Six participants were participating in order to predict the manner in which they did the exercise. There are five different ways how the manner can be performed: one correct and four manners with caracterized with the usual mistakes. The idea is to train four different models, bases on the four different methods: Decision Tree, Random Forest, Gradient Boosted Trees, Support Vector Machine using k-folds cross validation on the training set.

In the end, the prediction is performed using a validation set. It is randomly selected from the training data to obtain the accuracy and out of sample error rate. Based on those numbers, the best model is decided, and use it to predict remaining 20 cases using the test csv set.

This project is the final report for Coursera's Practical Machine Learning course, Data Science Specialization by John Hopkins.

## Description of the problem

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: (<http://groupware.les.inf.puc-rio.br/har>) (see the section on the Weight Lifting Exercise Dataset).

The training data for this project are available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here:

<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

The data for this project come from this source: <http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har>.

## Loading and cleaning the data

The data can be loaded as follows.

```{r, cache = TRUE}
url_training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url = url_training, destfile = "training.csv")
download.file(url = url_test, destfile = "test.csv")
training <- read.csv(file = "training.csv", header = TRUE, stringsAsFactors = TRUE)
testing <- read.csv(file = "test.csv", header = TRUE, stringsAsFactors = TRUE)
```

One can observe that there are 160 variables, 19622 observations in the training set and 20 observations in the test set. Now, let's prepare the training set for the validation. We divide the training set into a validation and a (sub) training set.

```{r}
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[inTrain, ]; myTesting <- training[-inTrain, ]
dim(myTraining); dim(myTesting)
```

If we inspect the training set, there is a lot of metadata and columns with multiple NA values. We can get rid of it and hope that it will not affect the analysis. Finally, we can get rid of variables that contain almost no variation.

```{r}
myDataNZV <- nearZeroVar(myTraining, saveMetrics=TRUE)
myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
myTraining <- myTraining[!myNZVvars]
dim(myTraining)
```
Delete the IDs. 

```{r}
myTraining <- myTraining[c(-1)]
```

Finally, we do the cleaning of variables with too many NAs. For Variables that have more than a 60% threshold of NAs, we're gonna leave them out. 

```{r}
trainingV3 <- myTraining #creating another subset to iterate in loop
for(i in 1:length(myTraining)) { #for every column in the training dataset
        if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 ) { #if n?? NAs > 60% of total observations
        for(j in 1:length(trainingV3)) {
            if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) ==1)  { #if the columns are the same:
                trainingV3 <- trainingV3[ , -j] #Remove that column
            }   
        } 
    }
}
#To check the new N?? of observations
dim(trainingV3)

myTraining <- trainingV3
rm(trainingV3)
```

Let's repeat the same thing with the test data. 

```{r}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting <- myTesting[clean1]
testing <- testing[clean2]
dim(myTesting)
dim(testing)
```

To make sure that the functioning of Decision Trees and especially RandomForest Algorithm with the Test data set (data set provided) will be done properly, we need to coerce the data into the same type.

```{r}
for (i in 1:length(testing) ) {
        for(j in 1:length(myTraining)) {
        if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(myTraining[i])
        }      
    }      
}
testing <- rbind(myTraining[2, -58] , testing) 
testing <- testing[-1,]
```

## Constructing models

We consider four different methods and models to fit the data. These are:

-   Decision Trees,
-   Random Forest,
-   Gradient Boosted Trees, and,
-   Support Vector Machines.

To control the train procedure, we set $k = 4$-fold cross-validation.

```{r}
control <- trainControl(method="cv", number=4, verboseIter=F)
```

### Decision Tree

The decision tree model is trained as follows.

```{r}
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
```

The prediction on the validation set is the following. Finally, we investigate the corresponding confusion matrix.

```{r}
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
confusionMatrix(predictionsA1, myTesting$classe)
```

The accuracy of the decision tree model is 0.865, which is not so satisfying.

### Random forest

The random forest model is trained as follows.

```{r}
modFitB1 <- randomForest(classe ~. , data=myTraining)
```

The prediction on the validation set is the following. Finally, we investigate the corresponding confusion matrix.

```{r}
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
confusionMatrix(predictionsB1, myTesting$classe)
```

The accuracy of the random forest model is 0.9982, which is great.
